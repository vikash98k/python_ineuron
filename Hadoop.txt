Big Data -> Data which can not be handled by traditional databases
-> Traditional database  system deals with structured data.
-> Big data system deals with structured, semi-structured,database, and unstructured data.
         -> 5v  - > 1) Volumne - huge amount of data (GB,TB)
                     2) Vairity - different format of data 
                        a) structure - rows and columns 
                        b) semi structure - schema is flexible (like json data,XML data)
                        c) un structure - does not have fixed representation 
                        like - image file ,audio file,video file
                    3) Velocity - speed of data generation
                        a) Batch processing -[Batch processing means we wait for some time window after we processing the data] electricity bill,credit cart bill
                        b) real time processing - [real time means every time you processing data] streaming,live gaming,feeds
                    4) Value - Extracting meaningful information
                    5) Veracity - related to uncertainty in the Data

Big Data Tools -> Databases [traditional,NoSql]
               -> Storage [HDFS] [Hadoop Distributed File System]
               -> processing [Batch and real] -> Batch -> Hadoop,Spark!!! Real ->spark flink
                   
               -> Data analysis [Data lakes, persistent Storage, data warehouse lakes(hive)]
               -> Messaging Queue [Kafka]
               -> Schedulers [Airflow]
               -> Cloud [Aws,Azure,GCP]

Hadoop -> Distributed Computation framework [specially design for batch data processing] 
       ->  it is used to efficiently store and process large datasets
       -> traditional processing approach

Vertical Scaling -> Adding extra capacity in existing machine [cannot extend on single system and due to some limitation like storage] 
Horizontal Scaling -> adding more machine in the system [master slave architecture]


Commodity Hardware -> simple machine which has storage and processing machine

Distributed storage -> Break data into small peaces and store it on different machine [cluster -combination of multiple machines]

Distributed Computation -> process multiple part of data on different machine at the same time

Hadoop Component and Architecture -> 
                 1) HDFS [store] [Hadoop Distributed file system]
                 2) Map reduce [processing]
                 3) Yarn [resource management]

HDFS -------------->
    -> HDFS is designed in such a way that it believes more in storing the data in a large chunk of blocks rather than storing small data blocks. 
    -> It mainly designed for working on Commodity Hardware devices(inexpensive devices), working on a Distributed file system design

Data storage Nodes in HDFS ------>

1) Name Node (master) -> Its work as master in a Hadoop cluster that guides the Datanode(slaves).
                      -> Its mainly used for storing MetaData(data about the data)
                      -> MetaData Its keep track the user's activity in a Hadoop cluster
                      -> Its manage Data Nodes 
                      -> Its record the MetaData of all the files in the cluster ex. the location of the blocks,size of the files,hierachy where replicated data is stored.
                      -> Namenode does not contain actual data of files.
FsImage - > File System Image
--->There are two files associated with the metadata. First is FsImage which contains the complete state of the file system namespace since the start of the NameNode.
    And second is EditLogs which contains all the recent modifications made to the file system with respect to the most recent FsImage.
    addtion of a new block, replication, deletion etc
    It records the changes since the last FsImage was created, it then merges the changes into the FsImage file to create a new FsImage file.
If one of the Datanodes gets failed in HDFS? --->
-> Namenode periodically receives a heartbeat and a Block report from each Datanode in the cluster
->When Namenode doesn’t receive any heartbeat message for 10 minutes(ByDefault) from a particular Datanode then corresponding Datanode is considered Dead or failed by Namenode

Block - > Its is smallest unit of physical memory where data is stored.

Block size -> Predefined size   
            Hadoop 2.x -> 128 MB
            Hadoop 1.x -> 64 MB

2) Data Node (slaves) -> Its are mainly utilized for storing the data in a Hadoop cluster.
                      -> The more number of DataNode, the Hadoop cluster will be able to store more data.
                      -> DataNode should have High storing capacity to store a large number of file blocks.

Replication in HDFS ---> Its ensures the availability of the data.
                      -> Replication is making a copy of something and the number of times you make a copy of that particular thing can be expressed as it’s Replication Factor.
                      -> HDFS stores the data in the form of various blocks at the same time Hadoop is also configured to make a copy of those file blocks.
                      -> By default, the Replication Factor for Hadoop is set to 3 which can be configured means you can change it manually as per your requirement.
                      -> Replication factor is nothing but it is process of making replicat or duplicate of data's
example - blocks - 4  then 4*3 = 12  blocks are made for the backup purpose

This is because for running Hadoop we are using commodity hardware (inexpensive system hardware) which can be crashed at any time.
We are not using the supercomputer for our Hadoop setup. 
That is why we need such a feature in HDFS which can make copies of that file blocks for backup purposes, this is known as fault tolerance. 
You can configure the Replication factor in your hdfs-site.xml file. 

Rack Awareness ---->The rack is nothing but just the physical collection of nodes in our Hadoop cluster (maybe 30 to 40).
                    A large Hadoop cluster is consists of so many Racks .
                    with the help of this Racks information Namenode chooses the closest Datanode to achieve the maximum performance while performing the read/write information which reduces the Network Traffic.
                    A rack can have multiple data nodes storing the file blocks and their replica’s.
                    More than 2 replica’s of a single block is not allowed on the same Rack.

Fault Tolerance ---->
                single point of time name node fails all communication will stop and whole Hadoop cluster will stop working
                to handle this situation we use backup or secondary name node


HDFS High Availability of Name node in Hadoop 2--->
-> 2 separate machine has been configured as Name nodes  where one is always in working state  and another is in standby state.
-> standby behaves as the slave and maintains enough state to provide a fast failover on working name node.

How HDFS achieves Fault Tolerance ->

1) replica’s mechanism
2) erasure coding - >
                --->save lots of space as compare replica’s mechanism
                ---> RAID(Redundant Array of Independant Disks) uses erasure coding.
                --->Erasure coding works by dividing files into small pieces and storing them on various disks.
                ---> For each strip of the original data set, a certain number of parity cells are calculated and stored.
                --->If any machine fails, the block can be recovered from the parity unit.Erasure coding reduces storage up to 50% 

There are two algorithms available in Erasure coding->

1) XOR Algorithm --->XOR operations can tolerate only one failure with n group size but we get the benefit of better storage efficiency by using XOR algorithm.
2) Reed Solomon Algorithm(improved EC algorithm) ---> In the uses linear algebra operations to generate multiple parity cells [multiple failure tolerance]
                                                 ---> efficiency -> (data cells)/(data cells + parity cells)


Yarn - Yet Another Resource Negotiator ------>Job Scheduling and Resource management

1) It was described as a Redesigned Resource Manager
2) IN Hadoop 1.0 version, the responsibility of Job tracker is split between the resource manager and application manager. 
3) YARN also allows different data processing engines like ->
                                                            Graph processing,Interactive processing,stream processing,batch processing

YARN feature->
            Scalability->allows Hadoop to extend and manage thousands of nodes and clusters.
            Compatibility-> it compatible with Hadoop 1.0[existing map reducing]
            Cluster Utilization -> YARN supports Dynamic utilization of cluster in Hadoop, which enables optimized Cluster Utilization
            Multi-tenancy -> It allows multiple engine access thus giving organizations a benefit of multi-tenancy.


The main components of YARN architecture------>

1) client -> It submits map-reduce jobs.
2) Resource Manager -> It is responsible for resource assignment and management among all the applications.
                    -> Whenever it receives a processing request, it forwards it to the corresponding node manager and allocates resources for the completion of the request accordingly
    components-> 
                a)Scheduler ->It performs scheduling based on the allocated application and available resources.
                b)Application manager ->It is responsible for accepting the application and negotiating the first container from the resource manager.
                                        It also restarts the Application Master container if a task fails.

3) Node Manager ->It take care of individual node on Hadoop cluster and manages application and workflow and that particular node
                ->It registers with the Resource Manager and sends heartbeats with the health status of the node.
                ->It monitors resource usage, performs log management and also kills a container based on directions from the resource manager.
                ->It is also responsible for creating the container process and start it on the request of Application master.

4) Application Master ->An application is a single job submitted to a framework
                      -> The application master is responsible for negotiating resources with the resource manager, tracking the status and monitoring progress of a single application.
                      ->The application master requests the container from the node manager by sending a Container Launch Context(CLC) which includes everything an application needs to run.
                      Once the application is started, it sends the health report to the resource manager from time-to-time

5) Container ->It is a collection of physical resources such as RAM, CPU cores and disk on a single node.
             -> The containers are invoked by Container Launch Context(CLC) which is a record that contains information such as environment variables, security tokens, dependencies etc.


------------Map reduce ------------------>
1) IT is based on Yarn framework
2)The major feature of MapReduce is to perform the distributed processing in parallel in a Hadoop cluster which Makes Hadoop working so fast
3) MapReduce has mainly 2 tasks which are divided phase-wise: 
   --->In first phase, Map is utilized and in next phase Reduce is utilized. 

->The Map() function here breaks this DataBlocks[input big data] into Tuples that are nothing but a key-value pair.
->These key-value pairs are now sent as input to the Reduce().
->The Reduce() function then combines this broken Tuples or key-value pair based on its Key value and form set of Tuples, and perform some operation like sorting, summation type job, etc.
->which is then sent to the final Output Node. Finally, the Output is Obtained. 

<----Map Task---->
Record Reader ->purpose break the records.
              ->It is responsible for providing key-value pairs in a Map() function.
              -> key - locational information
              -> value - data associated

Map ->a user-defined function whose work is to process the Tuples obtained from record reader
    ->The Map() function either does not generate any key-value pair or generate multiple pairs of these tuples.

Combiner ->It is used for grouping the data in the Map workflow.
         
Partitionar ->Partitional is responsible for fetching key-value pairs generated in the Mapper Phases.
            ->The partitioner generates the shards corresponding to each reducer.
            -> Then partitioner performs it’s(Hashcode) modulus with the number of reducers(key.hashcode()%(number of reducers)).

<----Reduce Task ---->

shuffle and sort -> The process in which the Mapper generates the intermediate key-value and transfers them to the Reducer task is known as Shuffling.
                 -> Using the Shuffling process the system can sort the data using its key value.
                
Reduce -> task of the Reduce is to gather the Tuple generated from Map and then perform some sorting and aggregation sort of process on those key-value depending on its key element.

OutputFormat -> Once all the operations are performed, the key-value pairs are written into the file with the help of record writer, each record in a new line, and the key and value in a space-separated manner.