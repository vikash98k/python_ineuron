Download Dataset 1 - https://drive.google.com/file/d/1WrG-9qv6atP-W3P_-gYln1hHyFKRKMHP/view

Download Dataset 2 - https://drive.google.com/file/d/1-JIPCZ34dyN6k9CqJa-Y8yxIGq6vTVXU/view


# 1. Create a schema based on the given dataset
     create database hive_projects
     use hive_project
    
     create table agent_loging_report
     (
     sl_no int,
     Agent string,
     `Date` string,
     Login_time string,
     Log_out string,
     Duration string
     )
     row format delimited
     fields terminated by ','
     tblproperties ("skip.header.line.count"="1");
    
    create table agent_performance
    (
    S_No int,
    `Date` string,
    Agent string,
    Total_chats int,
    avg_response_time string,
    avg_resolution_time string,
    avg_rating float,
    Total_feedback int
    )
    row format delimited
    fields terminated by ','
    tblproperties ("skip.header.line.count"="1");


# 2. Dump the data inside the hdfs in the given schema location.
     # creating directory inside hadoop
     hadoop fs -mkdir /hive_project_data
     
     # dumping data into hdfs
     hadoop fs -put /config/workspace/AgentLogingReport.csv /hive_project_data
     hadoop fs -put /config/workspace/AgentPerformance.csv /hive_project_data
     
     # loading data from hdfs to hive table
     load data inpath 'file:////config/workspace/AgentLogingReport.csv' into table agent_loging_report;
     load data inpath 'file:////config/workspace/AgentPerformance.csv' into table agent_performance;
    
# 3. List of all agents' names. 
     select Agent from agent_loging_report;
     
# 4. Find out agent average rating.
     select avg(avg_rating) from agent_performance;
     # answer: 1.4609629649255012
      
# 5. Total working days for each agents
     select count(distinct `Date`) from agent_loging_report;
     # answer: 13
      
# 6. Total query that each agent have taken
     select sum(Total_chats) from agent_performance;
     # answer: 14720
      
# 7. Total Feedback that each agent have received.
     select sum(Total_feedback) from agent_performance;
     # answer: 9259
     
# 8. Agent name who have average rating between 3.5 to 4 
     select Agent from agent_performance where avg_rating >=3.5 and  avg_rating<=4.0;
     
# 9. Agent name who have rating less than 3.5 
     select Agent from agent_performance where avg_rating < 3.5;
     
# 10. Agent name who have rating more than 4.5 
      select Agent from agent_performance where avg_rating > 4.5;
      
# 11. How many feedback agents have received more than 4.5 average.
      select sum(Total_feedback) from agent_performance where avg_rating > 4.5;
      # answer: 3489
       
# 12. Average weekly response time for each agent.
      set hive.groupby.position.alias = true;
      select Agent, date_format(`Date`,'W') week_no, 
      avg((split(avg_response_time ,':')[0]*3600 +split(avg_response_time ,':')[1]*60+split(avg_response_time ,':')[2] )/3600) avg_weekly_response_time
      from agent_performance group by 1,2;

      
# 13. Average weekly resolution time for each agents.
      set hive.groupby.position.alias = true;
      select Agent, date_format(`Date`,'W') week_no, 
      avg((split(avg_resolution_time,':')[0]*3600 +split(avg_resolution_time,':')[1]*60+split(avg_resolution_time,':')[2] )/3600) avg_weekly_resolution_time
      from agent_performance group by 1,2;
      
      
# 14. Find the number of chat on which they have received a feedback.
      select sum(Total_chats) from agent_performance where Total_feedback > 0;
      # answer : 14702
      
# 15. Total contribution hour for each and every agents weekly basis.
      set hive.groupby.position.alias = true;
      select Agent, date_format(`Date`,'W') week_no, 
      sum((split(Duration,':')[0]*3600 +split(Duration,':')[1]*60+split(Duration,':')[2] )/3600) total_weekly_contri_hrs
      from agent_loging_report group by 1,2;
      
# 16. Perform inner join, left join and right join based on the agent column and after joining the table export that data into your local system.
      # Right Join
      insert overwrite local directory '/config/workspace' row format delimited fields terminated by ',' 
      select * from agent_loging_report right join  agent_performance on agent_loging_report.Agent = agent_performance.Agent; 
      
      # Left Join
      insert overwrite local directory '/config/workspace' row format delimited fields terminated by ',' 
      select * from agent_loging_report left join  agent_performance on agent_loging_report.Agent = agent_performance.Agent;
       
      # Inner Join
      insert overwrite local directory '/config/workspace' row format delimited fields terminated by ',' 
      select * from agent_loging_report inner join  agent_performance on agent_loging_report.Agent = agent_performance.Agent; 
      
      # Outer Join
      insert overwrite local directory '/config/workspace' row format delimited fields terminated by ',' 
      select * from agent_loging_report outer join  agent_performance  on agent_loging_report.Agent = agent_performance.Agent;
      
      # for seeing the data we can use 'cat' command e.g. :
      cat /path_to_directory/file_name
      
# 17. Perform partitioning on top of the agent column and then on top of that perform bucketing for each partitioning.
      create table agent_loging_report_part
      (
      sl_no int,
      `Date` string,
      Loging_time string,
      Log_out string,
      Duration string
      )
      partitioned by (Agent string)
      clustered by (`Date`)
      sorted by (Duration)
      into 20 buckets
      row format delimited
      fields terminated by ','
      tblproperties ("skip.header.line.count"="1");
      
      # Checking the partition table into hdfs warehouse
      hadoop fs -ls /user/hive/warehouse/agent_loging_report_part
      

      
